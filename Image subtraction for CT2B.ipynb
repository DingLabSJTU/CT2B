{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image subtraction code for CT2B\n",
    "## Input images\n",
    "- `A1`: Time point 1; Green channel (AF488)\n",
    "- `B1`: Time point 1; Yellow channel (CY3)\n",
    "- `C1`: Time point 1; Red channel (CY5)\n",
    "- `D1`: Time point 1; Merged channel (AF488 + CY3 + CY5)\n",
    "-  \n",
    "- `A2`: Time point 2; Green channel (AF488)\n",
    "- `B2`: Time point 2; Yellow channel (CY3)\n",
    "- `C2`: Time point 2; Red channel (CY5)\n",
    "- `D2`: Time point 2; Merged channel (AF488 + CY3 + CY5)\n",
    "-    \n",
    "- `A3`: Time point 3; Green channel (AF488)\n",
    "- `B3`: Time point 3; Yellow channel (CY3)\n",
    "- `C3`: Time point 3; Red channel (CY5)\n",
    "- `D3`: Time point 3; Merged channel (AF488 + CY3 + CY5)\n",
    "\n",
    "Time point 1 is the reference image for alignment\n",
    "## Image alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded image from D://testcode//D1.tif with shape (1024, 1024, 3)\n",
      "Successfully loaded image from D://testcode//D2.tif with shape (1024, 1024, 3)\n",
      "Successfully loaded image from D://testcode//D3.tif with shape (1024, 1024, 3)\n",
      "Successfully loaded image from D://testcode//A2.tif with shape (1024, 1024, 3)\n",
      "Successfully loaded image from D://testcode//B2.tif with shape (1024, 1024, 3)\n",
      "Successfully loaded image from D://testcode//C2.tif with shape (1024, 1024, 3)\n",
      "Aligned channels for D2 successfully.\n",
      "Successfully loaded image from D://testcode//A3.tif with shape (1024, 1024, 3)\n",
      "Successfully loaded image from D://testcode//B3.tif with shape (1024, 1024, 3)\n",
      "Successfully loaded image from D://testcode//C3.tif with shape (1024, 1024, 3)\n",
      "Aligned channels for D3 successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}. Check the file path and integrity.\")\n",
    "    else:\n",
    "        # Optionally resize the image to reduce memory usage\n",
    "        print(f\"Successfully loaded image from {image_path} with shape {image.shape}\")\n",
    "    return image\n",
    "\n",
    "def align_images(base_image, target_image):\n",
    "    if base_image is None or target_image is None:\n",
    "        print(\"Error: One of the images is not loaded.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Convert images to grayscale\n",
    "        base_gray = cv2.cvtColor(base_image, cv2.COLOR_BGR2GRAY)\n",
    "        target_gray = cv2.cvtColor(target_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Initialize SIFT detector\n",
    "        sift = cv2.SIFT_create()\n",
    "\n",
    "        # Find the keypoints and descriptors with SIFT\n",
    "        kp1, des1 = sift.detectAndCompute(base_gray, None)\n",
    "        kp2, des2 = sift.detectAndCompute(target_gray, None)\n",
    "\n",
    "        # Check if descriptors are found\n",
    "        if des1 is None or des2 is None:\n",
    "            print(\"Error: Descriptors not found in one of the images.\")\n",
    "            return None, None\n",
    "\n",
    "        # FLANN parameters\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "\n",
    "        # Initialize FLANN matcher\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "        # Match descriptors using KNN\n",
    "        matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "        # Store good matches as per Lowe's ratio test.\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.7 * n.distance:\n",
    "                good_matches.append(m)\n",
    "\n",
    "        if len(good_matches) > 4:\n",
    "            src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "            dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "            # Calculate Homography\n",
    "            H, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "            # Warp target image to align with the base image\n",
    "            aligned_image = cv2.warpPerspective(target_image, H, (base_image.shape[1], base_image.shape[0]))\n",
    "            return aligned_image, H\n",
    "        else:\n",
    "            print(\"Error: Not enough matches found.\")\n",
    "            return None, None\n",
    "    except cv2.error as e:\n",
    "        print(f\"OpenCV error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load images\n",
    "D1 = load_image(r'D://testcode//D1.tif')\n",
    "D2 = load_image(r'D://testcode//D2.tif')\n",
    "D3 = load_image(r'D://testcode//D3.tif')\n",
    "\n",
    "# Align D2 and D3 to D1 if loaded successfully\n",
    "if D1 is not None and D2 is not None:\n",
    "    aligned_D2, H2 = align_images(D1, D2)\n",
    "    if aligned_D2 is None:\n",
    "        print(\"Failed to align D2. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "if D1 is not None and D3 is not None:\n",
    "    aligned_D3, H3 = align_images(D1, D3)\n",
    "    if aligned_D3 is None:\n",
    "        print(\"Failed to align D3. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "# Load channels for D2 (A2, B2, C2)\n",
    "A2 = load_image(r'D://testcode//A2.tif')\n",
    "B2 = load_image(r'D://testcode//B2.tif')\n",
    "C2 = load_image(r'D://testcode//C2.tif')\n",
    "\n",
    "if A2 is not None and B2 is not None and C2 is not None:\n",
    "    # Apply the same transformation to channels\n",
    "    aligned_A2 = cv2.warpPerspective(A2, H2, (D1.shape[1], D1.shape[0]))\n",
    "    aligned_B2 = cv2.warpPerspective(B2, H2, (D1.shape[1], D1.shape[0]))\n",
    "    aligned_C2 = cv2.warpPerspective(C2, H2, (D1.shape[1], D1.shape[0]))\n",
    "    print(\"Aligned channels for D2 successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load one or more channels for D2. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Load channels for D3 (A3, B3, C3)\n",
    "A3 = load_image(r'D://testcode//A3.tif')\n",
    "B3 = load_image(r'D://testcode//B3.tif')\n",
    "C3 = load_image(r'D://testcode//C3.tif')\n",
    "\n",
    "if A3 is not None and B3 is not None and C3 is not None:\n",
    "    # Apply the same transformation to channels\n",
    "    aligned_A3 = cv2.warpPerspective(A3, H3, (D1.shape[1], D1.shape[0]))\n",
    "    aligned_B3 = cv2.warpPerspective(B3, H3, (D1.shape[1], D1.shape[0]))\n",
    "    aligned_C3 = cv2.warpPerspective(C3, H3, (D1.shape[1], D1.shape[0]))\n",
    "    print(\"Aligned channels for D3 successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load one or more channels for D3. Exiting.\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save aligned image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned D2 saved as 'aligned_D2.jpg'.\n",
      "Aligned D3 saved as 'aligned_D3.jpg'.\n",
      "Aligned A2 saved as 'aligned_A2.jpg'.\n",
      "Aligned B2 saved as 'aligned_B2.jpg'.\n",
      "Aligned C2 saved as 'aligned_C2.jpg'.\n",
      "Aligned A3 saved as 'aligned_A3.jpg'.\n",
      "Aligned B3 saved as 'aligned_B3.jpg'.\n",
      "Aligned C3 saved as 'aligned_C3.jpg'.\n"
     ]
    }
   ],
   "source": [
    "# Save aligned images\n",
    "if aligned_D2 is not None:\n",
    "    cv2.imwrite('D://testcode//aligned_D2.tif', aligned_D2)\n",
    "    print(\"Aligned D2 saved as 'aligned_D2.jpg'.\")\n",
    "\n",
    "if aligned_D3 is not None:\n",
    "    cv2.imwrite('D://testcode//aligned_D3.tif', aligned_D3)\n",
    "    print(\"Aligned D3 saved as 'aligned_D3.jpg'.\")\n",
    "\n",
    "if aligned_A2 is not None:\n",
    "    cv2.imwrite('D://testcode//aligned_A2.tif', aligned_A2)\n",
    "    print(\"Aligned A2 saved as 'aligned_A2.jpg'.\")\n",
    "\n",
    "if aligned_B2 is not None:\n",
    "    cv2.imwrite('D://testcode//aligned_B2.tif', aligned_B2)\n",
    "    print(\"Aligned B2 saved as 'aligned_B2.jpg'.\")\n",
    "\n",
    "if aligned_C2 is not None:\n",
    "    cv2.imwrite('D://testcode//aligned_C2.tif', aligned_C2)\n",
    "    print(\"Aligned C2 saved as 'aligned_C2.jpg'.\")\n",
    "\n",
    "if aligned_A3 is not None:\n",
    "    cv2.imwrite('D://testcode//aligned_A3.tif', aligned_A3)\n",
    "    print(\"Aligned A3 saved as 'aligned_A3.jpg'.\")\n",
    "\n",
    "if aligned_B3 is not None:\n",
    "    cv2.imwrite('D://testcode//aligned_B3.tif', aligned_B3)\n",
    "    print(\"Aligned B3 saved as 'aligned_B3.jpg'.\")\n",
    "\n",
    "if aligned_C3 is not None:\n",
    "    cv2.imwrite('D://testcode//aligned_C3.tif', aligned_C3)\n",
    "    print(\"Aligned C3 saved as 'aligned_C3.jpg'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve proteins according to the pre-set color barcode \n",
    "### The following is an example for SV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import images\n",
    "ImageC1 = cv2.imread('D://testcode//C1.tif', cv2.IMREAD_GRAYSCALE)\n",
    "ImageC2 = cv2.imread('D://testcode//aligned_C2.tif', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get the size of each image\n",
    "height, width = ImageC1.shape\n",
    "\n",
    "# Create an empty matrix to store the new gray value\n",
    "new_gray_values = np.zeros((height, width), dtype=np.uint16)\n",
    "\n",
    "# Traverse each pixel coordinate\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        # Calculate the new gray value according to the pre-set color barcode (the following is just an example for SV2)\n",
    "        if ImageC1[i, j] > ImageC2[i, j]:\n",
    "            new_gray_values[i, j] = ImageC1[i, j] - ImageC2[i, j]\n",
    "        else:\n",
    "            new_gray_values[i, j] = 0\n",
    "# Convert the new grayscale value matrix to an image\n",
    "cv2.imwrite('D://testcode//SV2.tif', new_gray_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image display\n",
    "The decoded images were imported to zen 3.11 software for viewing. \n",
    "\n",
    "The grayscale image was assigned with pseudo color:  Open zen 3.11 → Dimensions → Channels.\n",
    "\n",
    "To merge images: Open zen 3.11 → Processing → Method → Add channes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
